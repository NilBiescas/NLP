{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm import Laplace, MLE, StupidBackoff\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"To Sherlock Holmes she is always ‘The Woman’. I have\n",
    "seldom heard him mention her under any other name.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the sentence\n",
    "sentence_token = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'Sherlock'),\n",
       " ('Sherlock', 'Holmes'),\n",
       " ('Holmes', 'she'),\n",
       " ('she', 'is'),\n",
       " ('is', 'always'),\n",
       " ('always', '‘'),\n",
       " ('‘', 'The'),\n",
       " ('The', 'Woman'),\n",
       " ('Woman', '’'),\n",
       " ('’', '.'),\n",
       " ('.', 'I'),\n",
       " ('I', 'have'),\n",
       " ('have', 'seldom'),\n",
       " ('seldom', 'heard'),\n",
       " ('heard', 'him'),\n",
       " ('him', 'mention'),\n",
       " ('mention', 'her'),\n",
       " ('her', 'under'),\n",
       " ('under', 'any'),\n",
       " ('any', 'other'),\n",
       " ('other', 'name'),\n",
       " ('name', '.')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the bigrams\n",
    "bin_grams  = list(nltk.ngrams(sentence_token, 2))\n",
    "bin_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To', 'Sherlock', 'Holmes'),\n",
       " ('Sherlock', 'Holmes', 'she'),\n",
       " ('Holmes', 'she', 'is'),\n",
       " ('she', 'is', 'always'),\n",
       " ('is', 'always', '‘'),\n",
       " ('always', '‘', 'The'),\n",
       " ('‘', 'The', 'Woman'),\n",
       " ('The', 'Woman', '’'),\n",
       " ('Woman', '’', '.'),\n",
       " ('’', '.', 'I'),\n",
       " ('.', 'I', 'have'),\n",
       " ('I', 'have', 'seldom'),\n",
       " ('have', 'seldom', 'heard'),\n",
       " ('seldom', 'heard', 'him'),\n",
       " ('heard', 'him', 'mention'),\n",
       " ('him', 'mention', 'her'),\n",
       " ('mention', 'her', 'under'),\n",
       " ('her', 'under', 'any'),\n",
       " ('under', 'any', 'other'),\n",
       " ('any', 'other', 'name'),\n",
       " ('other', 'name', '.')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the trigrams\n",
    "tri_grams = list(nltk.ngrams(sentence_token, 3))\n",
    "tri_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Tokens that we remove ----\n",
      " {'(', '?', '$', '=', ';', '!', '{', '>', '\"', '}', ')', '#', \"'\", ',', ':', '-', '_', '`', '|', '~', '&', '”', '\\\\', '+', '[', '*', '^', '%', '/', '.', '’', '<', ']', '@', '‘', '“'}\n",
      "\n",
      " To Sherlock Holmes she is always The Woman. I have seldom heard him mention her under any other name\n"
     ]
    }
   ],
   "source": [
    "# Remove special tokens\n",
    "import string\n",
    "print(\"---- Tokens that we remove ----\\n\", (set(string.punctuation) | set([\"'\", '”', '“', \"’\", \"‘\"])))\n",
    "sentence_token = [word for word in sentence_token if word not in (set(string.punctuation) | set([\"'\", '”', '“', \"’\", \"‘\"]))]\n",
    "print(\"\\n\", \" \".join(sentence_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('To', 'Sherlock'), 1), (('Sherlock', 'Holmes'), 1), (('Holmes', 'she'), 1), (('she', 'is'), 1), (('is', 'always'), 1), (('always', 'The'), 1), (('The', 'Woman.'), 1), (('Woman.', 'I'), 1), (('I', 'have'), 1), (('have', 'seldom'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Computing the bigrams\n",
    "bigrams = list(nltk.ngrams(sentence_token, 2))\n",
    "# The frequency of each bigram\n",
    "bigrams_count = Counter(bigrams)\n",
    "print(bigrams_count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common 2-gram are: \n",
      " [(('of', 'the'), 1895), (('in', 'the'), 1139), (('to', 'the'), 733), (('from', 'the'), 437), (('of', 'his'), 371), (('and', 'the'), 365), (('of', 'a'), 337), (('on', 'the'), 337), (('the', 'whale'), 325), (('with', 'the'), 325)]\n",
      "The 10 most common 3-gram are: \n",
      " [(('of', 'the', 'whale'), 88), (('the', 'Sperm', 'Whale'), 77), (('the', 'White', 'Whale'), 62), (('one', 'of', 'the'), 61), (('of', 'the', 'sea'), 57), (('out', 'of', 'the'), 54), (('part', 'of', 'the'), 53), (('the', 'ship', 's'), 52), (('a', 'sort', 'of'), 49), (('the', 'whale', 's'), 48)]\n",
      "The 10 most common 4-gram are: \n",
      " [(('of', 'the', 'Sperm', 'Whale'), 30), (('at', 'the', 'same', 'time'), 20), (('of', 'the', 'whale', 's'), 17), (('the', 'bottom', 'of', 'the'), 17), (('the', 'Sperm', 'Whale', 's'), 17), (('the', 'old', 'man', 's'), 15), (('Project', 'Gutenberg', 'Literary', 'Archive'), 13), (('in', 'the', 'act', 'of'), 12), (('Project', 'Gutenberg-tm', 'electronic', 'works'), 12), (('Gutenberg', 'Literary', 'Archive', 'Foundation'), 12)]\n"
     ]
    }
   ],
   "source": [
    "with open('moby_dick.txt', 'r') as f:\n",
    "    moby = f.read()\n",
    "\n",
    "def get_n_gram_statistics(n_gram:int = 2):\n",
    "    moby_token = nltk.word_tokenize(moby)\n",
    "    moby_token = [word for word in moby_token if word not in (set(string.punctuation) | set([\"'\", '”', '“', \"’\", \"‘\"]))]\n",
    "    ngrams = list(nltk.ngrams(moby_token, n_gram))\n",
    "    ngrams_count = Counter(ngrams)\n",
    "    return ngrams_count.most_common(10)\n",
    "\n",
    "print(\"The 10 most common 2-gram are: \\n\", get_n_gram_statistics(2))\n",
    "print(\"The 10 most common 3-gram are: \\n\", get_n_gram_statistics(3))\n",
    "print(\"The 10 most common 4-gram are: \\n\", get_n_gram_statistics(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Ngrams\n",
    "\n",
    "#apuntar paraules predites i comparar amb reals, casos bons i dolents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line num:  16\n"
     ]
    }
   ],
   "source": [
    "# Loading Sherlock holmes book\n",
    "with open('Adventures_Holmes.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "# Optaining the first paragraph for test data\n",
    "lines = lines[28:]\n",
    "test_paragraph = []\n",
    "new_paragraph = []\n",
    "for i in range(len(lines)- 1):\n",
    "    new_paragraph.append(lines[i][:-1])\n",
    "    if lines[i][-1] == '\\n' and lines[i + 1] == '\\n':\n",
    "        test_paragraph.append(\" \".join(new_paragraph))\n",
    "        print(\"Line num: \", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence of the training data: \n",
      " Adler, of dubious and questionable memory.\n"
     ]
    }
   ],
   "source": [
    "n_grams_size = 4\n",
    "# Prepare the training data\n",
    "\n",
    "# Join all the lines together\n",
    "train_data = \" \".join(lines[16:])\n",
    "\n",
    "# Separate the data into sentences\n",
    "train_sentences = nltk.sent_tokenize(train_data)\n",
    "print(\"The first sentence of the training data: \\n\", train_sentences[0])\n",
    "\n",
    "# Tokenize the sentences\n",
    "train_tokens = [nltk.word_tokenize(sentence) for sentence in train_sentences]\n",
    "\n",
    "# Adding padding to the tokens and then computing the n-grams and the vocabulary\n",
    "train, vocab = padded_everygram_pipeline(n_grams_size, train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lenguage model and train it\n",
    "model = StupidBackoff(order=n_grams_size, alpha=0.4)\n",
    "model.fit(train, vocabulary_text=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is:  10006\n"
     ]
    }
   ],
   "source": [
    "list_vocab = list(model.vocab)\n",
    "print(\"The vocabulary size is: \", len(list_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate test paragraph into sentences\n",
    "test_sentences = nltk.sent_tokenize(test_paragraph[0])\n",
    "\n",
    "# Tokenizing each sentence in the test data and adding padding\n",
    "test_tokens = [pad_both_ends(nltk.word_tokenize(sentence), n=n_grams_size) for sentence in test_sentences]\n",
    "\n",
    "# Generating n-grams for each sentence in the test data\n",
    "test_n_grams = [list(nltk.ngrams(sentence, n_grams_size)) for sentence in test_tokens]\n",
    "\n",
    "# Flatten the list of n-grams\n",
    "test_n_grams_2 = []\n",
    "for sentence in test_n_grams:\n",
    "        test_n_grams_2 += sentence\n",
    "test_n_grams = test_n_grams_2\n",
    "\n",
    "# Removing n-grams that contain '</s>' as we don't want to test the prediction of the end of the sentence\n",
    "test_n_grams_2 = []\n",
    "for n_gram in test_n_grams:\n",
    "        if '</s>' not in n_gram:\n",
    "                test_n_grams_2.append(n_gram)\n",
    "test_n_grams = test_n_grams_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picking the most probable word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is:  0.1608695652173913\n",
      "The top 3 accuracy is:  0.26956521739130435\n"
     ]
    }
   ],
   "source": [
    "# Remove special tokens from the vocabulary list\n",
    "if \"<s>\" in list_vocab:\n",
    "    list_vocab.remove(\"<s>\")\n",
    "if \"</s>\" in list_vocab:\n",
    "    list_vocab.remove(\"</s>\")\n",
    "\n",
    "correct = []\n",
    "wrong = []\n",
    "correct_count = 0\n",
    "correct_count_top3 = 0\n",
    "total_count = 0\n",
    "all_predicted = []\n",
    "# Prediting the next word for each n-gram in the test data\n",
    "for n_gram in test_n_grams:\n",
    "    # print(n_gram)\n",
    "    # print(n_gram[:-1])\n",
    "\n",
    "    # Iterating over the vocabulary to find the word with the highest probability\n",
    "    # We also store the 3 words with the highest probability\n",
    "    dict_prob = {}\n",
    "    for word in list_vocab:\n",
    "        prob = model.unmasked_score(word, context=n_gram[:-1])\n",
    "        dict_prob[word] = prob\n",
    "    \n",
    "    top_3 = sorted(dict_prob, key=dict_prob.get, reverse=True)[:3]\n",
    "    predicted_word = top_3[0]\n",
    "    # print(\"The predicted word is: \", predicted_word)\n",
    "    # print(\"The top 3 words are: \", top_3)\n",
    "    all_predicted.append(predicted_word)\n",
    "    if predicted_word == n_gram[-1]:\n",
    "        correct.append(n_gram)\n",
    "        correct_count += 1\n",
    "    if n_gram[-1] in top_3:\n",
    "        correct_count_top3 += 1\n",
    "    else:\n",
    "        wrong.append((n_gram, predicted_word))\n",
    "    total_count += 1\n",
    "\n",
    "print(\"The accuracy is: \", correct_count/total_count)\n",
    "print(\"The top 3 accuracy is: \", correct_count_top3/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the correct predictions are: \n",
      " [('<s>', 'To', 'Sherlock', 'Holmes'), ('is', 'always', '_the_', 'woman'), ('always', '_the_', 'woman', '.'), ('<s>', '<s>', 'I', 'have'), ('her', 'under', 'any', 'other'), ('<s>', '<s>', 'It', 'was'), ('any', 'emotion', 'akin', 'to'), ('love', 'for', 'Irene', 'Adler'), ('<s>', 'All', 'emotions', ','), ('All', 'emotions', ',', 'and')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Some of the correct predictions are: \\n\", correct[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the wrong predictions are:\n",
      "The n-gram is:  ('<s>', '<s>', '<s>', 'To')\n",
      "The word predicted:  “\n",
      "The n-gram is:  ('<s>', '<s>', 'To', 'Sherlock')\n",
      "The word predicted:  me\n",
      "The n-gram is:  ('To', 'Sherlock', 'Holmes', 'she')\n",
      "The word predicted:  ,\n",
      "The n-gram is:  ('Sherlock', 'Holmes', 'she', 'is')\n",
      "The word predicted:  bade\n",
      "The n-gram is:  ('Holmes', 'she', 'is', 'always')\n",
      "The word predicted:  not\n",
      "The n-gram is:  ('she', 'is', 'always', '_the_')\n",
      "The word predicted:  of\n",
      "The n-gram is:  ('<s>', 'I', 'have', 'seldom')\n",
      "The word predicted:  no\n",
      "The n-gram is:  ('I', 'have', 'seldom', 'heard')\n",
      "The word predicted:  seen\n",
      "The n-gram is:  ('have', 'seldom', 'heard', 'him')\n",
      "The word predicted:  of\n",
      "The n-gram is:  ('seldom', 'heard', 'him', 'mention')\n",
      "The word predicted:  do\n"
     ]
    }
   ],
   "source": [
    "print(\"Some of the wrong predictions are:\")\n",
    "for i in range(10):\n",
    "    n_gram, predicted_word = wrong[i]\n",
    "    print(\"The n-gram is: \", n_gram)\n",
    "    print(\"The word predicted: \", predicted_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him mention her \n",
      "under any other name. In his eyes she eclipses and predominates the whole of her s\n",
      "ex. It was not that he felt any emotion akin to love for Irene Adler. All emotions\n",
      ", and that one particularly, were abhorrent to his cold, precise but admirably bal\n",
      "anced mind. He was, I take it, the most perfect reasoning and observing machine th\n",
      "at the world has seen, but as a lover he would have placed himself in a false posi\n",
      "tion. He never spoke of the softer passions, save with a gibe and a sneer. They we\n",
      "re admirable things for the observer—excellent for drawing the veil from men’s mot\n",
      "ives and actions. But for the trained reasoner to admit such intrusions into his o\n",
      "wn delicate and finely adjusted temperament was to introduce a distracting factor \n",
      "which might throw a doubt upon all his mental results. Grit in a sensitive instrum\n",
      "ent, or a crack in one of his own high-power lenses, would not be more disturbing \n",
      "than a strong emotion in a nature such as his. And yet there was but one woman to \n",
      "him, and that woman was the late Irene Adler, of dubious and questionable memory.\n"
     ]
    }
   ],
   "source": [
    "text = test_paragraph[0]\n",
    "\n",
    "printable_text = \"\"\n",
    "for i in range(len(text)):\n",
    "    if i % 82 == 0:\n",
    "        printable_text += '\\n'\n",
    "    printable_text += text[i]\n",
    "\n",
    "print(printable_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted text with 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "“ me Holmes, bade not of woman. “ have no seen of do of, the other work, “ the s\n",
      "ingular. had, I,, house that own, “ was a the of had that other. to bad him. the\n",
      " Adler, “ the, and I the of to and sufficient, the feet, Mr. as I done,, “ was a\n",
      " as have it, then leather extraordinary happiness, extraordinary the and I door.\n",
      " been very and I I rule ; had not been before to the perplexing alarm in “ was d\n",
      "id, the nervous,, and that a very, I few. “ were all queen which some best, the \n",
      "of,, the, s heads were I. “ I the best as, the that a, the chair, for I, that, a\n",
      " be you little, in I have up little as that that strength,, “, the few,, and the\n",
      " means, the of the keen,,, and you have very than, the precious nature. the few \n",
      "of a I client “ now I was an one way, bear, and that the ’ standing name Ezekiah\n",
      " Adler, as dubious and questionable memory.\n"
     ]
    }
   ],
   "source": [
    "pred_text = \" \".join(all_predicted).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "printable_text = \"\"\n",
    "for i in range(len(pred_text)):\n",
    "    if i % 80 == 0:\n",
    "        printable_text += '\\n'\n",
    "    printable_text += pred_text[i]\n",
    "\n",
    "print(printable_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
